The purpose of this lab was to achieve parallelism with map reduce by analyzing the Kaggle Airline Delay dataset to observe and analyze various delays, such as the average total late aircraft delay and the longest delays due to security and weather.
Since we want to achieve multiple executions using one program and a huge dataset, we can break the tasks into processes, hence we incorporate multiple threads/processes in the computer simultaneously.
The first part of the lab was completed on the tux server and the programs are written in python. 
Each question from a-d can be compiled in its own program followed by its respective letter. In writing this program sequentially for nodes of 2,4,8, and 16, the larger the number of nodes the faster the program ran. 
The csv file used was the 'DelayedFlights.csv'. For part 2, we ran the file 'pi.py' in PySpark- "execfile('pi.py')" to parallelize the map/reduce to algorithim to compute pi. Similar to part 1, we noticed that it was faster to compute the pi vales with more nodes than less. We tested the program with each number of nodes provided but specified 8 nodes for the makefile. I made an error for part 2, i did not check the references link, so I initially made the lab much harder for me than it should have been, I actually implemented the Monte-Carlo algorithm. Also, I'm not sure about whether the makfile works, but if you want you can run the individual files with passing the the two arguments being the .csv file and number of Nodes you want running.

